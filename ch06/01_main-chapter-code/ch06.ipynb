{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c024bfa4-1a7a-4751-b5a1-827225a3478b",
   "metadata": {
    "id": "c024bfa4-1a7a-4751-b5a1-827225a3478b"
   },
   "source": [
    "<table style=\"width:100%\">\n",
    "<tr>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<font size=\"2\">\n",
    "Supplementary code for the <a href=\"http://mng.bz/orYv\">Build a Large Language Model From Scratch</a> book by <a href=\"https://sebastianraschka.com\">Sebastian Raschka</a><br>\n",
    "<br>Code repository: <a href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</a>\n",
    "</font>\n",
    "</td>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<a href=\"http://mng.bz/orYv\"><img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/cover-small.webp\" width=\"100px\"></a>\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfabadb8-5935-45ff-b39c-db7a29012129",
   "metadata": {
    "id": "bfabadb8-5935-45ff-b39c-db7a29012129"
   },
   "source": [
    "# Chapter 6: Finetuning for Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b7e01c2-1c84-4f2a-bb51-2e0b74abda90",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5b7e01c2-1c84-4f2a-bb51-2e0b74abda90",
    "outputId": "9495f150-9d79-4910-d6e7-6c0d9aae4a41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matplotlib version: 3.10.3\n",
      "numpy version: 2.0.2\n",
      "tiktoken version: 0.9.0\n",
      "torch version: 2.7.0+cu128\n",
      "tensorflow version: 2.18.1\n",
      "pandas version: 2.2.3\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\"matplotlib\",  # Plotting library\n",
    "        \"numpy\",       # PyTorch & TensorFlow dependency\n",
    "        \"tiktoken\",    # Tokenizer\n",
    "        \"torch\",       # Deep learning library\n",
    "        \"tensorflow\",  # For OpenAI's pretrained weights\n",
    "        \"pandas\"       # Dataset loading\n",
    "       ]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a445828a-ff10-4efa-9f60-a2e2aed4c87d",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch06_compressed/chapter-overview.webp\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a84cf35-b37f-4c15-8972-dfafc9fadc1c",
   "metadata": {
    "id": "3a84cf35-b37f-4c15-8972-dfafc9fadc1c"
   },
   "source": [
    "## 6.1 Different categories of finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede3d731-5123-4f02-accd-c670ce50a5a3",
   "metadata": {
    "id": "ede3d731-5123-4f02-accd-c670ce50a5a3"
   },
   "source": [
    "- No code in this section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac45579d-d485-47dc-829e-43be7f4db57b",
   "metadata": {},
   "source": [
    "- The most common ways to finetune language models are instruction-finetuning and classification finetuning\n",
    "- Instruction-finetuning, depicted below, is the topic of the next chapter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c29ef42-46d9-43d4-8bb4-94974e1665e4",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch06_compressed/instructions.webp\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f60321-95b8-46a9-97bf-1d07fda2c3dd",
   "metadata": {},
   "source": [
    "- Classification finetuning, the topic of this chapter, is a procedure you may already be familiar with if you have a background in machine learning -- it's similar to training a convolutional network to classify handwritten digits, for example\n",
    "- In classification finetuning, we have a specific number of class labels (for example, \"spam\" and \"not spam\") that the model can output\n",
    "- A classification finetuned model can only predict classes it has seen during training (for example, \"spam\" or \"not spam\"), whereas an instruction-finetuned model can usually perform many tasks\n",
    "- We can think of a classification-finetuned model as a very specialized model; in practice, it is much easier to create a specialized model than a generalist model that performs well on many different tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b37a0c4-0bb1-4061-b1fe-eaa4416d52c3",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch06_compressed/spam-non-spam.webp\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7017a2-32aa-4002-a2f3-12aac293ccdf",
   "metadata": {
    "id": "8c7017a2-32aa-4002-a2f3-12aac293ccdf"
   },
   "source": [
    "## 6.2 Preparing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f628975-d2e8-4f7f-ab38-92bb868b7067",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch06_compressed/overview-1.webp\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbd459f-63fa-4d8c-8499-e23103156c7d",
   "metadata": {
    "id": "9fbd459f-63fa-4d8c-8499-e23103156c7d"
   },
   "source": [
    "- This section prepares the dataset we use for classification finetuning\n",
    "- We use a dataset consisting of spam and non-spam text messages to finetune the LLM to classify them\n",
    "- First, we download and unzip the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "def7c09b-af9c-4216-90ce-5e67aed1065c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "def7c09b-af9c-4216-90ce-5e67aed1065c",
    "outputId": "424e4423-f623-443c-ab9e-656f9e867559"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded and saved as sms_spam_collection/SMSSpamCollection.tsv\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import zipfile\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "url = \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\n",
    "zip_path = \"sms_spam_collection.zip\"\n",
    "extracted_path = \"sms_spam_collection\"\n",
    "data_file_path = Path(extracted_path) / \"SMSSpamCollection.tsv\"\n",
    "\n",
    "def download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path):\n",
    "    if data_file_path.exists():\n",
    "        print(f\"{data_file_path} already exists. Skipping download and extraction.\")\n",
    "        return\n",
    "\n",
    "    # Downloading the file\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        with open(zip_path, \"wb\") as out_file:\n",
    "            out_file.write(response.read())\n",
    "\n",
    "    # Unzipping the file\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(extracted_path)\n",
    "\n",
    "    # Add .tsv file extension\n",
    "    original_file_path = Path(extracted_path) / \"SMSSpamCollection\"\n",
    "    os.rename(original_file_path, data_file_path)\n",
    "    print(f\"File downloaded and saved as {data_file_path}\")\n",
    "\n",
    "try:\n",
    "    download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)\n",
    "except (urllib.error.HTTPError, urllib.error.URLError, TimeoutError) as e:\n",
    "    print(f\"Primary URL failed: {e}. Trying backup URL...\")\n",
    "    url = \"https://f001.backblazeb2.com/file/LLMs-from-scratch/sms%2Bspam%2Bcollection.zip\"\n",
    "    download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aac2d19-06d0-4005-916b-0bd4b1ee50d1",
   "metadata": {
    "id": "6aac2d19-06d0-4005-916b-0bd4b1ee50d1"
   },
   "source": [
    "- The dataset is saved as a tab-separated text file, which we can load into a pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da0ed4da-ac31-4e4d-8bdd-2153be4656a4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "da0ed4da-ac31-4e4d-8bdd-2153be4656a4",
    "outputId": "a16c5cde-d341-4887-a93f-baa9bec542ab"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Label",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Text",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "b8a00def-a190-4e85-8574-11124bdf042a",
       "rows": [
        [
         "0",
         "ham",
         "Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat..."
        ],
        [
         "1",
         "ham",
         "Ok lar... Joking wif u oni..."
        ],
        [
         "2",
         "spam",
         "Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's"
        ],
        [
         "3",
         "ham",
         "U dun say so early hor... U c already then say..."
        ],
        [
         "4",
         "ham",
         "Nah I don't think he goes to usf, he lives around here though"
        ],
        [
         "5",
         "spam",
         "FreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, Â£1.50 to rcv"
        ],
        [
         "6",
         "ham",
         "Even my brother is not like to speak with me. They treat me like aids patent."
        ],
        [
         "7",
         "ham",
         "As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your callertune for all Callers. Press *9 to copy your friends Callertune"
        ],
        [
         "8",
         "spam",
         "WINNER!! As a valued network customer you have been selected to receivea Â£900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only."
        ],
        [
         "9",
         "spam",
         "Had your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free! Call The Mobile Update Co FREE on 08002986030"
        ],
        [
         "10",
         "ham",
         "I'm gonna be home soon and i don't want to talk about this stuff anymore tonight, k? I've cried enough today."
        ],
        [
         "11",
         "spam",
         "SIX chances to win CASH! From 100 to 20,000 pounds txt> CSH11 and send to 87575. Cost 150p/day, 6days, 16+ TsandCs apply Reply HL 4 info"
        ],
        [
         "12",
         "spam",
         "URGENT! You have won a 1 week FREE membership in our Â£100,000 Prize Jackpot! Txt the word: CLAIM to No: 81010 T&C www.dbuk.net LCCLTD POBOX 4403LDNW1A7RW18"
        ],
        [
         "13",
         "ham",
         "I've been searching for the right words to thank you for this breather. I promise i wont take your help for granted and will fulfil my promise. You have been wonderful and a blessing at all times."
        ],
        [
         "14",
         "ham",
         "I HAVE A DATE ON SUNDAY WITH WILL!!"
        ],
        [
         "15",
         "spam",
         "XXXMobileMovieClub: To use your credit, click the WAP link in the next txt message or click here>> http://wap. xxxmobilemovieclub.com?n=QJKGIGHJJGCBL"
        ],
        [
         "16",
         "ham",
         "Oh k...i'm watching here:)"
        ],
        [
         "17",
         "ham",
         "Eh u remember how 2 spell his name... Yes i did. He v naughty make until i v wet."
        ],
        [
         "18",
         "ham",
         "Fine if thatÂs the way u feel. ThatÂs the way its gota b"
        ],
        [
         "19",
         "spam",
         "England v Macedonia - dont miss the goals/team news. Txt ur national team to 87077 eg ENGLAND to 87077 Try:WALES, SCOTLAND 4txt/Ãº1.20 POBOXox36504W45WQ 16+"
        ],
        [
         "20",
         "ham",
         "Is that seriously how you spell his name?"
        ],
        [
         "21",
         "ham",
         "Iâm going to try for 2 months ha ha only joking"
        ],
        [
         "22",
         "ham",
         "So Ã¼ pay first lar... Then when is da stock comin..."
        ],
        [
         "23",
         "ham",
         "Aft i finish my lunch then i go str down lor. Ard 3 smth lor. U finish ur lunch already?"
        ],
        [
         "24",
         "ham",
         "Ffffffffff. Alright no way I can meet up with you sooner?"
        ],
        [
         "25",
         "ham",
         "Just forced myself to eat a slice. I'm really not hungry tho. This sucks. Mark is getting worried. He knows I'm sick when I turn down pizza. Lol"
        ],
        [
         "26",
         "ham",
         "Lol your always so convincing."
        ],
        [
         "27",
         "ham",
         "Did you catch the bus ? Are you frying an egg ? Did you make a tea? Are you eating your mom's left over dinner ? Do you feel my Love ?"
        ],
        [
         "28",
         "ham",
         "I'm back &amp; we're packing the car now, I'll let you know if there's room"
        ],
        [
         "29",
         "ham",
         "Ahhh. Work. I vaguely remember that! What does it feel like? Lol"
        ],
        [
         "30",
         "ham",
         "Wait that's still not all that clear, were you not sure about me being sarcastic or that that's why x doesn't want to live with us"
        ],
        [
         "31",
         "ham",
         "Yeah he got in at 2 and was v apologetic. n had fallen out and she was actin like spoilt child and he got caught up in that. Till 2! But we won't go there! Not doing too badly cheers. You? "
        ],
        [
         "32",
         "ham",
         "K tell me anything about you."
        ],
        [
         "33",
         "ham",
         "For fear of fainting with the of all that housework you just did? Quick have a cuppa"
        ],
        [
         "34",
         "spam",
         "Thanks for your subscription to Ringtone UK your mobile will be charged Â£5/month Please confirm by replying YES or NO. If you reply NO you will not be charged"
        ],
        [
         "35",
         "ham",
         "Yup... Ok i go home look at the timings then i msg Ã¼ again... Xuhui going to learn on 2nd may too but her lesson is at 8am"
        ],
        [
         "36",
         "ham",
         "Oops, I'll let you know when my roommate's done"
        ],
        [
         "37",
         "ham",
         "I see the letter B on my car"
        ],
        [
         "38",
         "ham",
         "Anything lor... U decide..."
        ],
        [
         "39",
         "ham",
         "Hello! How's you and how did saturday go? I was just texting to see if you'd decided to do anything tomo. Not that i'm trying to invite myself or anything!"
        ],
        [
         "40",
         "ham",
         "Pls go ahead with watts. I just wanted to be sure. Do have a great weekend. Abiola"
        ],
        [
         "41",
         "ham",
         "Did I forget to tell you ? I want you , I need you, I crave you ... But most of all ... I love you my sweet Arabian steed ... Mmmmmm ... Yummy"
        ],
        [
         "42",
         "spam",
         "07732584351 - Rodger Burns - MSG = We tried to call you re your reply to our sms for a free nokia mobile + free camcorder. Please call now 08000930705 for delivery tomorrow"
        ],
        [
         "43",
         "ham",
         "WHO ARE YOU SEEING?"
        ],
        [
         "44",
         "ham",
         "Great! I hope you like your man well endowed. I am  &lt;#&gt;  inches..."
        ],
        [
         "45",
         "ham",
         "No calls..messages..missed calls"
        ],
        [
         "46",
         "ham",
         "Didn't you get hep b immunisation in nigeria."
        ],
        [
         "47",
         "ham",
         "Fair enough, anything going on?"
        ],
        [
         "48",
         "ham",
         "Yeah hopefully, if tyler can't do it I could maybe ask around a bit"
        ],
        [
         "49",
         "ham",
         "U don't know how stubborn I am. I didn't even want to go to the hospital. I kept telling Mark I'm not a weak sucker. Hospitals are for weak suckers."
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5572
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will Ã¼ b going to esplanade fr home?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows Ã 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Label                                               Text\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...\n",
       "...    ...                                                ...\n",
       "5567  spam  This is the 2nd time we have tried 2 contact u...\n",
       "5568   ham               Will Ã¼ b going to esplanade fr home?\n",
       "5569   ham  Pity, * was in mood for that. So...any other s...\n",
       "5570   ham  The guy did some bitching but I acted like i'd...\n",
       "5571   ham                         Rofl. Its true to its name\n",
       "\n",
       "[5572 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(data_file_path, sep=\"\\t\", header=None, names=[\"Label\", \"Text\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b6e631-4f0b-4aab-82b9-8898e6663109",
   "metadata": {
    "id": "e7b6e631-4f0b-4aab-82b9-8898e6663109"
   },
   "source": [
    "- When we check the class distribution, we see that the data contains \"ham\" (i.e., \"not spam\") much more frequently than \"spam\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "495a5280-9d7c-41d4-9719-64ab99056d4c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "495a5280-9d7c-41d4-9719-64ab99056d4c",
    "outputId": "761e0482-43ba-4f46-f4b7-6774dae51b38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "ham     4825\n",
      "spam     747\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df[\"Label\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f773f054-0bdc-4aad-bbf6-397621bf63db",
   "metadata": {
    "id": "f773f054-0bdc-4aad-bbf6-397621bf63db"
   },
   "source": [
    "- For simplicity, and because we prefer a small dataset for educational purposes anyway (it will make it possible to finetune the LLM faster), we subsample (undersample) the dataset so that it contains 747 instances from each class\n",
    "- (Next to undersampling, there are several other ways to deal with class balances, but they are out of the scope of a book on LLMs; you can find examples and more information in the [`imbalanced-learn` user guide](https://imbalanced-learn.org/stable/user_guide.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7be4a0a2-9704-4a96-b38f-240339818688",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7be4a0a2-9704-4a96-b38f-240339818688",
    "outputId": "396dc415-cb71-4a88-e85d-d88201c6d73f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "ham     747\n",
      "spam    747\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def create_balanced_dataset(df):\n",
    "    \n",
    "    # Count the instances of \"spam\"\n",
    "    num_spam = df[df[\"Label\"] == \"spam\"].shape[0]\n",
    "    \n",
    "    # Randomly sample \"ham\" instances to match the number of \"spam\" instances\n",
    "    ham_subset = df[df[\"Label\"] == \"ham\"].sample(num_spam, random_state=123)\n",
    "    \n",
    "    # Combine ham \"subset\" with \"spam\"\n",
    "    balanced_df = pd.concat([ham_subset, df[df[\"Label\"] == \"spam\"]])\n",
    "\n",
    "    return balanced_df\n",
    "\n",
    "\n",
    "balanced_df = create_balanced_dataset(df)\n",
    "print(balanced_df[\"Label\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fd2f5a-06d8-4d30-a2e3-230b86c559d6",
   "metadata": {
    "id": "d3fd2f5a-06d8-4d30-a2e3-230b86c559d6"
   },
   "source": [
    "- Next, we change the string class labels \"ham\" and \"spam\" into integer class labels 0 and 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1b10c3d-5d57-42d0-8de8-cf80a06f5ffd",
   "metadata": {
    "id": "c1b10c3d-5d57-42d0-8de8-cf80a06f5ffd"
   },
   "outputs": [],
   "source": [
    "balanced_df[\"Label\"] = balanced_df[\"Label\"].map({\"ham\": 0, \"spam\": 1})    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6f7f062-ef4e-4020-8275-71990cab4414",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Label",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Text",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "bd9f9e2c-5a68-43f4-a857-59bd2f36dd4d",
       "rows": [
        [
         "4307",
         "0",
         "Awww dat is sweet! We can think of something to do he he! Have a nice time tonight ill probably txt u later cos im lonely :( xxx."
        ],
        [
         "4138",
         "0",
         "Just got to  &lt;#&gt;"
        ],
        [
         "4831",
         "0",
         "The word \"Checkmate\" in chess comes from the Persian phrase \"Shah Maat\" which means; \"the king is dead..\" Goodmorning.. Have a good day..:)"
        ],
        [
         "4461",
         "0",
         "This is wishing you a great day. Moji told me about your offer and as always i was speechless. You offer so easily to go to great lengths on my behalf and its stunning. My exam is next friday. After that i will keep in touch more. Sorry."
        ],
        [
         "5440",
         "0",
         "Thank you. do you generally date the brothas?"
        ],
        [
         "4448",
         "0",
         "Please tell me you have some of that special stock you were talking about"
        ],
        [
         "2603",
         "0",
         "So when you gonna get rimac access "
        ],
        [
         "2219",
         "0",
         "Nice talking to you! please dont forget my pix :) i want to see all of you..."
        ],
        [
         "5219",
         "0",
         "Pls she needs to dat slowly or she will vomit more."
        ],
        [
         "1035",
         "0",
         "ZOE IT JUST HIT ME 2 IM FUCKING SHITIN MYSELF IL DEFO TRY MY HARDEST 2 CUM 2MOROW LUV U MILLIONS LEKDOG"
        ],
        [
         "3515",
         "0",
         "I always chat with you. In fact i need money can you raise me?"
        ],
        [
         "3115",
         "0",
         "Yes watching footie but worried we're going to blow it - Phil Neville?"
        ],
        [
         "3553",
         "0",
         "Lol u still feeling sick?"
        ],
        [
         "5264",
         "0",
         "Storming msg: Wen u lift d phne, u say \"HELLO\" Do u knw wt is d real meaning of HELLO?? . . . It's d name of a girl..! . . . Yes.. And u knw who is dat girl?? \"Margaret Hello\" She is d girlfrnd f Grahmbell who invnted telphone... . . . . Moral:One can 4get d name of a person, bt not his girlfrnd... G o o d n i g h t . . .@"
        ],
        [
         "5539",
         "0",
         "Just sleeping..and surfing"
        ],
        [
         "2008",
         "0",
         "Hi here. have birth at on the  to  at 8lb 7oz. Mother and baby doing brilliantly."
        ],
        [
         "291",
         "0",
         "Hey you told your name to gautham ah?"
        ],
        [
         "3305",
         "0",
         "IM GONNAMISSU SO MUCH!!I WOULD SAY IL SEND U A POSTCARD BUTTHERES ABOUTAS MUCH CHANCE OF MEREMEMBERIN ASTHERE IS OFSI NOT BREAKIN HIS CONTRACT!! LUV Yaxx"
        ],
        [
         "3812",
         "0",
         "Excellent! Wish we were together right now!"
        ],
        [
         "4175",
         "0",
         "And pls pls drink plenty plenty water"
        ],
        [
         "4937",
         "0",
         "K..k.:)congratulation .."
        ],
        [
         "3986",
         "0",
         "Whatever, juliana. Do whatever you want."
        ],
        [
         "1859",
         "0",
         "Sir, i am waiting for your call."
        ],
        [
         "4495",
         "0",
         "Man this bus is so so so slow. I think you're gonna get there before me"
        ],
        [
         "5387",
         "0",
         "I will be gentle baby! Soon you will be taking all  &lt;#&gt;  inches deep inside your tight pussy..."
        ],
        [
         "1987",
         "0",
         "S..antha num corrct dane"
        ],
        [
         "4812",
         "0",
         "E admin building there? I might b slightly earlier... I'll call u when i'm reaching..."
        ],
        [
         "1906",
         "0",
         "There're some people by mu, I'm at the table by lambda"
        ],
        [
         "5124",
         "0",
         "He is impossible to argue with and he always treats me like his sub, like he never released me ... Which he did and I will remind him of that if necessary"
        ],
        [
         "1280",
         "0",
         "Waiting 4 my tv show 2 start lor... U leh still busy doing ur report?"
        ],
        [
         "3019",
         "0",
         "I didn't get the second half of that message"
        ],
        [
         "4617",
         "0",
         "Ã called dad oredi..."
        ],
        [
         "3434",
         "0",
         "Christmas is An occasion that is Celebrated as a Reflection of UR... Values..., Desires..., Affections...&amp; Traditions.... Have an ideal Christmas..."
        ],
        [
         "3052",
         "0",
         "Awesome question with a cute answer: Someone asked a boy \"how is ur life?\" . . He smiled &amp; answered: . . \"She is fine!\" Gudnite"
        ],
        [
         "2813",
         "0",
         "Say this slowly.? GOD,I LOVE YOU &amp; I NEED YOU,CLEAN MY HEART WITH YOUR BLOOD.Send this to Ten special people &amp; u c miracle tomorrow, do it,pls,pls do it..."
        ],
        [
         "2645",
         "0",
         "My friends use to call the same."
        ],
        [
         "774",
         "0",
         "I wil be there with in  &lt;#&gt;  minutes. Got any space"
        ],
        [
         "2904",
         "0",
         "Tell me pa. How is pain de."
        ],
        [
         "1102",
         "0",
         "Yeah go on then, bored and depressed sittin waitin for phone to ring... Hope the wind drops though, scary"
        ],
        [
         "3431",
         "0",
         "You've always been the brainy one."
        ],
        [
         "3234",
         "0",
         "Height of recycling: Read twice- People spend time for earning money and the same money is spent for spending time!;-) Good morning.. keep smiling:-)"
        ],
        [
         "5001",
         "0",
         "Well its not like you actually called someone a punto. That woulda been worse."
        ],
        [
         "4893",
         "0",
         "Miserable. They don't tell u that the side effects of birth control are massive gut wrenching cramps for the first 2 months. I didn't sleep at all last night."
        ],
        [
         "4850",
         "0",
         "either way works for me. I am  &lt;#&gt;  years old. Hope that doesnt bother you."
        ],
        [
         "411",
         "0",
         "Come by our room at some point so we can iron out the plan for this weekend"
        ],
        [
         "23",
         "0",
         "Aft i finish my lunch then i go str down lor. Ard 3 smth lor. U finish ur lunch already?"
        ],
        [
         "1825",
         "0",
         "Sent me ur email id soon"
        ],
        [
         "4478",
         "0",
         "Oh :-)only 4 outside players allowed to play know"
        ],
        [
         "1883",
         "0",
         "Sorry, I can't help you on this."
        ],
        [
         "287",
         "0",
         "Ok.."
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 1494
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4307</th>\n",
       "      <td>0</td>\n",
       "      <td>Awww dat is sweet! We can think of something t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4138</th>\n",
       "      <td>0</td>\n",
       "      <td>Just got to  &amp;lt;#&amp;gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4831</th>\n",
       "      <td>0</td>\n",
       "      <td>The word \"Checkmate\" in chess comes from the P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4461</th>\n",
       "      <td>0</td>\n",
       "      <td>This is wishing you a great day. Moji told me ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5440</th>\n",
       "      <td>0</td>\n",
       "      <td>Thank you. do you generally date the brothas?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5537</th>\n",
       "      <td>1</td>\n",
       "      <td>Want explicit SEX in 30 secs? Ring 02073162414...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5540</th>\n",
       "      <td>1</td>\n",
       "      <td>ASKED 3MOBILE IF 0870 CHATLINES INCLU IN FREE ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5547</th>\n",
       "      <td>1</td>\n",
       "      <td>Had your contract mobile 11 Mnths? Latest Moto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5566</th>\n",
       "      <td>1</td>\n",
       "      <td>REMINDER FROM O2: To get 2.50 pounds free call...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>1</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1494 rows Ã 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Label                                               Text\n",
       "4307      0  Awww dat is sweet! We can think of something t...\n",
       "4138      0                             Just got to  &lt;#&gt;\n",
       "4831      0  The word \"Checkmate\" in chess comes from the P...\n",
       "4461      0  This is wishing you a great day. Moji told me ...\n",
       "5440      0      Thank you. do you generally date the brothas?\n",
       "...     ...                                                ...\n",
       "5537      1  Want explicit SEX in 30 secs? Ring 02073162414...\n",
       "5540      1  ASKED 3MOBILE IF 0870 CHATLINES INCLU IN FREE ...\n",
       "5547      1  Had your contract mobile 11 Mnths? Latest Moto...\n",
       "5566      1  REMINDER FROM O2: To get 2.50 pounds free call...\n",
       "5567      1  This is the 2nd time we have tried 2 contact u...\n",
       "\n",
       "[1494 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5715e685-35b4-4b45-a86c-8a8694de9d6f",
   "metadata": {
    "id": "5715e685-35b4-4b45-a86c-8a8694de9d6f"
   },
   "source": [
    "- Let's now define a function that randomly divides the dataset into training, validation, and test subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "uQl0Psdmx15D",
   "metadata": {
    "id": "uQl0Psdmx15D"
   },
   "outputs": [],
   "source": [
    "def random_split(df, train_frac, validation_frac):\n",
    "    # Shuffle the entire DataFrame\n",
    "    df = df.sample(frac=1, random_state=123).reset_index(drop=True)\n",
    "\n",
    "    # Calculate split indices\n",
    "    train_end = int(len(df) * train_frac)\n",
    "    validation_end = train_end + int(len(df) * validation_frac)\n",
    "\n",
    "    # Split the DataFrame\n",
    "    train_df = df[:train_end]\n",
    "    validation_df = df[train_end:validation_end]\n",
    "    test_df = df[validation_end:]\n",
    "\n",
    "    return train_df, validation_df, test_df\n",
    "\n",
    "train_df, validation_df, test_df = random_split(balanced_df, 0.7, 0.1)\n",
    "# Test size is implied to be 0.2 as the remainder\n",
    "\n",
    "train_df.to_csv(\"train.csv\", index=None)\n",
    "validation_df.to_csv(\"validation.csv\", index=None)\n",
    "test_df.to_csv(\"test.csv\", index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d7a0c5-1d5f-458a-b685-3f49520b0094",
   "metadata": {},
   "source": [
    "## 6.3 Creating data loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7126108a-75e7-4862-b0fb-cbf59a18bb6c",
   "metadata": {
    "id": "7126108a-75e7-4862-b0fb-cbf59a18bb6c"
   },
   "source": [
    "- Note that the text messages have different lengths; if we want to combine multiple training examples in a batch, we have to either\n",
    "  1. truncate all messages to the length of the shortest message in the dataset or batch\n",
    "  2. pad all messages to the length of the longest message in the dataset or batch\n",
    "\n",
    "- We choose option 2 and pad all messages to the longest message in the dataset\n",
    "- For that, we use `<|endoftext|>` as a padding token, as discussed in chapter 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0829f33f-1428-4f22-9886-7fee633b3666",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch06_compressed/pad-input-sequences.webp?123\" width=500px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74c3c463-8763-4cc0-9320-41c7eaad8ab7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "74c3c463-8763-4cc0-9320-41c7eaad8ab7",
    "outputId": "b5b48439-32c8-4b37-cca2-c9dc8fa86563"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50256]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "print(tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f582ff-68bf-450e-bd87-5fb61afe431c",
   "metadata": {
    "id": "04f582ff-68bf-450e-bd87-5fb61afe431c"
   },
   "source": [
    "- The `SpamDataset` class below identifies the longest sequence in the training dataset and adds the padding token to the others to match that sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7791b52-af18-4ac4-afa9-b921068e383e",
   "metadata": {
    "id": "d7791b52-af18-4ac4-afa9-b921068e383e"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class SpamDataset(Dataset):\n",
    "    def __init__(self, csv_file, tokenizer, max_length=None, pad_token_id=50256):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "\n",
    "        # Pre-tokenize texts\n",
    "        self.encoded_texts = [\n",
    "            tokenizer.encode(text) for text in self.data[\"Text\"]\n",
    "        ]\n",
    "\n",
    "        if max_length is None:\n",
    "            self.max_length = self._longest_encoded_length()\n",
    "        else:\n",
    "            self.max_length = max_length\n",
    "            # Truncate sequences if they are longer than max_length\n",
    "            self.encoded_texts = [\n",
    "                encoded_text[:self.max_length]\n",
    "                for encoded_text in self.encoded_texts\n",
    "            ]\n",
    "\n",
    "        # Pad sequences to the longest sequence\n",
    "        self.encoded_texts = [\n",
    "            encoded_text + [pad_token_id] * (self.max_length - len(encoded_text))\n",
    "            for encoded_text in self.encoded_texts\n",
    "        ]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        encoded = self.encoded_texts[index]\n",
    "        label = self.data.iloc[index][\"Label\"]\n",
    "        return (\n",
    "            torch.tensor(encoded, dtype=torch.long),\n",
    "            torch.tensor(label, dtype=torch.long)\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def _longest_encoded_length(self):\n",
    "        max_length = 0\n",
    "        for encoded_text in self.encoded_texts:\n",
    "            encoded_length = len(encoded_text)\n",
    "            if encoded_length > max_length:\n",
    "                max_length = encoded_length\n",
    "        return max_length\n",
    "        # Note: A more pythonic version to implement this method\n",
    "        # is the following, which is also used in the next chapter:\n",
    "        # return max(len(encoded_text) for encoded_text in self.encoded_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "uzj85f8ou82h",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uzj85f8ou82h",
    "outputId": "d08f1cf0-c24d-445f-a3f8-793532c3716f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n"
     ]
    }
   ],
   "source": [
    "train_dataset = SpamDataset(\n",
    "    csv_file=\"train.csv\",\n",
    "    max_length=None,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "print(train_dataset.max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bdd932-97eb-4b88-9cf9-d766ea4c3a60",
   "metadata": {},
   "source": [
    "- We also pad the validation and test set to the longest training sequence\n",
    "- Note that validation and test set samples that are longer than the longest training example are being truncated via `encoded_text[:self.max_length]` in the `SpamDataset` code\n",
    "- This behavior is entirely optional, and it would also work well if we set `max_length=None` in both the validation and test set cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb0c502d-a75e-4248-8ea0-196e2b00c61e",
   "metadata": {
    "id": "bb0c502d-a75e-4248-8ea0-196e2b00c61e"
   },
   "outputs": [],
   "source": [
    "val_dataset = SpamDataset(\n",
    "    csv_file=\"validation.csv\",\n",
    "    max_length=train_dataset.max_length,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "test_dataset = SpamDataset(\n",
    "    csv_file=\"test.csv\",\n",
    "    max_length=train_dataset.max_length,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20170d89-85a0-4844-9887-832f5d23432a",
   "metadata": {},
   "source": [
    "- Next, we use the dataset to instantiate the data loaders, which is similar to creating the data loaders in previous chapters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bcc349-205f-48f8-9655-95ff21f5e72f",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch06_compressed/batch.webp\" width=500px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8681adc0-6f02-4e75-b01a-a6ab75d05542",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8681adc0-6f02-4e75-b01a-a6ab75d05542",
    "outputId": "3266c410-4fdb-4a8c-a142-7f707e2525ab"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "num_workers = 0\n",
    "batch_size = 8\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7335db-e0bb-4e27-80c5-eea11e593a57",
   "metadata": {},
   "source": [
    "- As a verification step, we iterate through the data loaders and ensure that the batches contain 8 training examples each, where each training example consists of 120 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4dee6882-4c3a-4964-af15-fa31f86ad047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "Input batch dimensions: torch.Size([8, 120])\n",
      "Label batch dimensions torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for input_batch, target_batch in train_loader:\n",
    "    pass\n",
    "\n",
    "print(\"Input batch dimensions:\", input_batch.shape)\n",
    "print(\"Label batch dimensions\", target_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdd7947-7039-49bf-8a5e-c0a2f4281ca1",
   "metadata": {},
   "source": [
    "- Lastly, let's print the total number of batches in each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "IZfw-TYD2zTj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IZfw-TYD2zTj",
    "outputId": "6934bbf2-9797-4fbe-d26b-1a246e18c2fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130 training batches\n",
      "19 validation batches\n",
      "38 test batches\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(train_loader)} training batches\")\n",
    "print(f\"{len(val_loader)} validation batches\")\n",
    "print(f\"{len(test_loader)} test batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c4f61a-5f5d-4b3b-97cf-151b617d1d6c",
   "metadata": {
    "id": "d1c4f61a-5f5d-4b3b-97cf-151b617d1d6c"
   },
   "source": [
    "## 6.4 Initializing a model with pretrained weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e1af8b-8bd1-4b44-8b8b-dc031496e208",
   "metadata": {},
   "source": [
    "- In this section, we initialize the pretrained model we worked with in the previous chapter\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch06_compressed/overview-2.webp\" width=500px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2992d779-f9fb-4812-a117-553eb790a5a9",
   "metadata": {
    "id": "2992d779-f9fb-4812-a117-553eb790a5a9"
   },
   "outputs": [],
   "source": [
    "CHOOSE_MODEL = \"gpt2-small (124M)\"\n",
    "INPUT_PROMPT = \"Every effort moves\"\n",
    "\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,     # Vocabulary size\n",
    "    \"context_length\": 1024,  # Context length\n",
    "    \"drop_rate\": 0.0,        # Dropout rate\n",
    "    \"qkv_bias\": True         # Query-key-value bias\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "\n",
    "assert train_dataset.max_length <= BASE_CONFIG[\"context_length\"], (\n",
    "    f\"Dataset length {train_dataset.max_length} exceeds model's context \"\n",
    "    f\"length {BASE_CONFIG['context_length']}. Reinitialize data sets with \"\n",
    "    f\"`max_length={BASE_CONFIG['context_length']}`\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "022a649a-44f5-466c-8a8e-326c063384f5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "022a649a-44f5-466c-8a8e-326c063384f5",
    "outputId": "7091e401-8442-4f47-a1d9-ecb42a1ef930"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2/124M/checkpoint\n",
      "File already exists and is up-to-date: gpt2/124M/encoder.json\n",
      "File already exists and is up-to-date: gpt2/124M/hparams.json\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2/124M/vocab.bpe\n"
     ]
    }
   ],
   "source": [
    "from gpt_download import download_and_load_gpt2\n",
    "from previous_chapters import GPTModel, load_weights_into_gpt\n",
    "# If the `previous_chapters.py` file is not available locally,\n",
    "# you can import it from the `llms-from-scratch` PyPI package.\n",
    "# For details, see: https://github.com/rasbt/LLMs-from-scratch/tree/main/pkg\n",
    "# E.g.,\n",
    "# from llms_from_scratch.ch04 import GPTModel\n",
    "# from llms_from_scratch.ch05 import download_and_load_gpt2, load_weights_into_gpt\n",
    "\n",
    "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "settings, params = download_and_load_gpt2(model_size=model_size, models_dir=\"gpt2\")\n",
    "\n",
    "model = GPTModel(BASE_CONFIG)\n",
    "load_weights_into_gpt(model, params)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8e056c-abe0-415f-b34d-df686204259e",
   "metadata": {},
   "source": [
    "- To ensure that the model was loaded correctly, let's double-check that it generates coherent text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d8ac25ff-74b1-4149-8dc5-4c429d464330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you forward.\n",
      "\n",
      "The first step is to understand the importance of your work\n"
     ]
    }
   ],
   "source": [
    "from previous_chapters import (\n",
    "    generate_text_simple,\n",
    "    text_to_token_ids,\n",
    "    token_ids_to_text\n",
    ")\n",
    "\n",
    "# Alternatively:\n",
    "# from llms_from_scratch.ch05 import (\n",
    "#    generate_text_simple,\n",
    "#    text_to_token_ids,\n",
    "#    token_ids_to_text\n",
    "# )\n",
    "\n",
    "\n",
    "text_1 = \"Every effort moves you\"\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(text_1, tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=BASE_CONFIG[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69162550-6a02-4ece-8db1-06c71d61946f",
   "metadata": {},
   "source": [
    "- Before we finetune the model as a classifier, let's see if the model can perhaps already classify spam messages via prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "94224aa9-c95a-4f8a-a420-76d01e3a800c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the following text 'spam'? Answer with 'yes' or 'no': 'You are a winner you have been specially selected to receive $1000 cash or a $2000 award.'\n",
      "\n",
      "The following text 'spam'? Answer with 'yes' or 'no': 'You are a winner\n"
     ]
    }
   ],
   "source": [
    "text_2 = (\n",
    "    \"Is the following text 'spam'? Answer with 'yes' or 'no':\"\n",
    "    \" 'You are a winner you have been specially\"\n",
    "    \" selected to receive $1000 cash or a $2000 award.'\"\n",
    ")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(text_2, tokenizer),\n",
    "    max_new_tokens=23,\n",
    "    context_size=BASE_CONFIG[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce39ed0-2c77-410d-8392-dd15d4b22016",
   "metadata": {},
   "source": [
    "- As we can see, the model is not very good at following instructions\n",
    "- This is expected, since it has only been pretrained and not instruction-finetuned (instruction finetuning will be covered in the next chapter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9ae440-32f9-412f-96cf-fd52cc3e2522",
   "metadata": {
    "id": "4c9ae440-32f9-412f-96cf-fd52cc3e2522"
   },
   "source": [
    "## 6.5 Adding a classification head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e9d66f-76b2-40fc-9ec5-3f972a8db9c0",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch06_compressed/lm-head.webp\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217bac05-78df-4412-bd80-612f8061c01d",
   "metadata": {},
   "source": [
    "- In this section, we are modifying the pretrained LLM to make it ready for classification finetuning\n",
    "- Let's take a look at the model architecture first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b23aff91-6bd0-48da-88f6-353657e6c981",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1d8f7a01-b7c0-48d4-b1e7-8c12cc7ad932",
    "outputId": "b6a5b9b5-a92f-498f-d7cb-b58dd99e4497"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTModel(\n",
      "  (tok_emb): Embedding(50257, 768)\n",
      "  (pos_emb): Embedding(1024, 768)\n",
      "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
      "  (trf_blocks): Sequential(\n",
      "    (0): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (1): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (2): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (3): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (4): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (5): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (6): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (7): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (8): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (9): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (10): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (11): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (final_norm): LayerNorm()\n",
      "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f640a76-dd00-4769-9bc8-1aed0cec330d",
   "metadata": {},
   "source": [
    "- Above, we can see the architecture we implemented in chapter 4 neatly laid out\n",
    "- The goal is to replace and finetune the output layer\n",
    "- To achieve this, we first freeze the model, meaning that we make all layers non-trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fkMWFl-0etea",
   "metadata": {
    "id": "fkMWFl-0etea"
   },
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72155f83-87d9-476a-a978-a15aa2d44147",
   "metadata": {},
   "source": [
    "- Then, we replace the output layer (`model.out_head`), which originally maps the layer inputs to 50,257 dimensions (the size of the vocabulary)\n",
    "- Since we finetune the model for binary classification (predicting 2 classes, \"spam\" and \"not spam\"), we can replace the output layer as shown below, which will be trainable by default\n",
    "- Note that we use `BASE_CONFIG[\"emb_dim\"]` (which is equal to 768 in the `\"gpt2-small (124M)\"` model) to keep the code below more general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7e759fa0-0f69-41be-b576-17e5f20e04cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "num_classes = 2\n",
    "model.out_head = torch.nn.Linear(in_features=BASE_CONFIG[\"emb_dim\"], out_features=num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30be5475-ae77-4f97-8f3e-dec462b1339f",
   "metadata": {},
   "source": [
    "- Technically, it's sufficient to only train the output layer\n",
    "- However, as I found in [Finetuning Large Language Models](https://magazine.sebastianraschka.com/p/finetuning-large-language-models), experiments show that finetuning additional layers can noticeably improve the performance\n",
    "- So, we are also making the last transformer block and the final `LayerNorm` module connecting the last transformer block to the output layer trainable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be7c1eb-c46c-4065-8525-eea1b8c66d10",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch06_compressed/trainable.webp\" width=500px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2aedc120-5ee3-48f6-92f2-ad9304ebcdc7",
   "metadata": {
    "id": "2aedc120-5ee3-48f6-92f2-ad9304ebcdc7"
   },
   "outputs": [],
   "source": [
    "for param in model.trf_blocks[-1].parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for param in model.final_norm.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f012b899-8284-4d3a-97c0-8a48eb33ba2e",
   "metadata": {},
   "source": [
    "- We can still use this model similar to before in previous chapters\n",
    "- For example, let's feed it some text input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f645c06a-7df6-451c-ad3f-eafb18224ebc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f645c06a-7df6-451c-ad3f-eafb18224ebc",
    "outputId": "27e041b1-d731-48a1-cf60-f22d4565304e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: tensor([[5211,  345,  423,  640]])\n",
      "Inputs dimensions: torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer.encode(\"Do you have time\")\n",
    "inputs = torch.tensor(inputs).unsqueeze(0)\n",
    "print(\"Inputs:\", inputs)\n",
    "print(\"Inputs dimensions:\", inputs.shape) # shape: (batch_size, num_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbf8481-772d-467b-851c-a62b86d0cb1b",
   "metadata": {},
   "source": [
    "- What's different compared to previous chapters is that it now has two output dimensions instead of 50,257"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "48dc84f1-85cc-4609-9cee-94ff539f00f4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "48dc84f1-85cc-4609-9cee-94ff539f00f4",
    "outputId": "9cae7448-253d-4776-973e-0af190b06354"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs:\n",
      " tensor([[[-1.5854,  0.9904],\n",
      "         [-3.7235,  7.4548],\n",
      "         [-2.2661,  6.6049],\n",
      "         [-3.5983,  3.9902]]])\n",
      "Outputs dimensions: torch.Size([1, 4, 2])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(inputs)\n",
    "\n",
    "print(\"Outputs:\\n\", outputs)\n",
    "print(\"Outputs dimensions:\", outputs.shape) # shape: (batch_size, num_tokens, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75430a01-ef9c-426a-aca0-664689c4f461",
   "metadata": {},
   "source": [
    "- As discussed in previous chapters, for each input token, there's one output vector\n",
    "- Since we fed the model a text sample with 4 input tokens, the output consists of 4 2-dimensional output vectors above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df9144f-6817-4be4-8d4b-5d4dadfe4a9b",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch06_compressed/input-and-output.webp\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bb8616-c791-4f5c-bac0-5302f663e46a",
   "metadata": {},
   "source": [
    "- In chapter 3, we discussed the attention mechanism, which connects each input token to each other input token\n",
    "- In chapter 3, we then also introduced the causal attention mask that is used in GPT-like models; this causal mask lets a current token only attend to the current and previous token positions\n",
    "- Based on this causal attention mechanism, the 4th (last) token contains the most information among all tokens because it's the only token that includes information about all other tokens\n",
    "- Hence, we are particularly interested in this last token, which we will finetune for the spam classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "49383a8c-41d5-4dab-98f1-238bca0c2ed7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "49383a8c-41d5-4dab-98f1-238bca0c2ed7",
    "outputId": "e79eb155-fa1f-46ed-ff8c-d828c3a3fabd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last output token: tensor([[-3.5983,  3.9902]])\n"
     ]
    }
   ],
   "source": [
    "print(\"Last output token:\", outputs[:, -1, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df08ae0-e664-4670-b7c5-8a2280d9b41b",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch06_compressed/attention-mask.webp\" width=200px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32aa4aef-e1e9-491b-9adf-5aa973e59b8c",
   "metadata": {},
   "source": [
    "## 6.6 Calculating the classification loss and accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669e1fd1-ace8-44b4-b438-185ed0ba8b33",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch06_compressed/overview-3.webp?1\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7df4ee-0a34-4a4d-896d-affbbf81e0b3",
   "metadata": {},
   "source": [
    "- Before explaining the loss calculation, let's have a brief look at how the model outputs are turned into class labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557996dd-4c6b-49c4-ab83-f60ef7e1d69e",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch06_compressed/class-argmax.webp\" width=600px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c77faab1-3461-4118-866a-6171f2b89aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last output token: tensor([[-3.5983,  3.9902]])\n"
     ]
    }
   ],
   "source": [
    "print(\"Last output token:\", outputs[:, -1, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edd71fa-628a-4d00-b81d-6d8bcb2c341d",
   "metadata": {},
   "source": [
    "- Similar to chapter 5, we convert the outputs (logits) into probability scores via the `softmax` function and then obtain the index position of the largest probability value via the `argmax` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b81efa92-9be1-4b9e-8790-ce1fc7b17f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class label: 1\n"
     ]
    }
   ],
   "source": [
    "probas = torch.softmax(outputs[:, -1, :], dim=-1)\n",
    "label = torch.argmax(probas)\n",
    "print(\"Class label:\", label.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414a6f02-307e-4147-a416-14d115bf8179",
   "metadata": {},
   "source": [
    "- Note that the softmax function is optional here, as explained in chapter 5, because the largest outputs correspond to the largest probability scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f9f9ad66-4969-4501-8239-3ccdb37e71a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class label: 1\n"
     ]
    }
   ],
   "source": [
    "logits = outputs[:, -1, :]\n",
    "label = torch.argmax(logits)\n",
    "print(\"Class label:\", label.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb20d3a-cbba-4ab1-8584-d94e16589505",
   "metadata": {},
   "source": [
    "- We can apply this concept to calculate the so-called classification accuracy, which computes the percentage of correct predictions in a given dataset\n",
    "- To calculate the classification accuracy, we can apply the preceding `argmax`-based prediction code to all examples in a dataset and calculate the fraction of correct predictions as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3ecf9572-aed0-4a21-9c3b-7f9f2aec5f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy_loader(data_loader, model, device, num_batches=None):\n",
    "    model.eval()\n",
    "    correct_predictions, num_examples = 0, 0\n",
    "\n",
    "    if num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits = model(input_batch)[:, -1, :]  # Logits of last output token\n",
    "            predicted_labels = torch.argmax(logits, dim=-1)\n",
    "\n",
    "            num_examples += predicted_labels.shape[0]\n",
    "            correct_predictions += (predicted_labels == target_batch).sum().item()\n",
    "        else:\n",
    "            break\n",
    "    return correct_predictions / num_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7165fe46-a284-410b-957f-7524877d1a1a",
   "metadata": {},
   "source": [
    "- Let's apply the function to calculate the classification accuracies for the different datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390e5255-8427-488c-adef-e1c10ab4fb26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cuda device.\n",
      "Training accuracy: 46.25%\n",
      "Validation accuracy: 45.00%\n",
      "Test accuracy: 48.75%\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Note:\n",
    "# Uncommenting the following lines will allow the code to run on Apple Silicon chips, if applicable,\n",
    "# which is approximately 2x faster than on an Apple CPU (as measured on an M3 MacBook Air).\n",
    "# As of this writing, in PyTorch 2.4, the results obtained via CPU and MPS were identical.\n",
    "# However, in earlier versions of PyTorch, you may observe different results when using MPS.\n",
    "\n",
    "#if torch.cuda.is_available():\n",
    "#    device = torch.device(\"cuda\")\n",
    "#elif torch.backends.mps.is_available():\n",
    "#    device = torch.device(\"mps\")\n",
    "#else:\n",
    "#    device = torch.device(\"cpu\")\n",
    "#print(f\"Running on {device} device.\")\n",
    "\n",
    "model.to(device) # no assignment model = model.to(device) necessary for nn.Module classes\n",
    "\n",
    "torch.manual_seed(123) # For reproducibility due to the shuffling in the training data loader\n",
    "\n",
    "train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=10)\n",
    "val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=10)\n",
    "test_accuracy = calc_accuracy_loader(test_loader, model, device, num_batches=10)\n",
    "\n",
    "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30345e2a-afed-4d22-9486-f4010f90a871",
   "metadata": {},
   "source": [
    "- As we can see, the prediction accuracies are not very good, since we haven't finetuned the model, yet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4a9d15-8fc7-48a2-8734-d92a2f265328",
   "metadata": {},
   "source": [
    "- Before we can start finetuning (/training), we first have to define the loss function we want to optimize during training\n",
    "- The goal is to maximize the spam classification accuracy of the model; however, classification accuracy is not a differentiable function\n",
    "- Hence, instead, we minimize the cross-entropy loss as a proxy for maximizing the classification accuracy (you can learn more about this topic in lecture 8 of my freely available [Introduction to Deep Learning](https://sebastianraschka.com/blog/2021/dl-course.html#l08-multinomial-logistic-regression--softmax-regression) class)\n",
    "\n",
    "- The `calc_loss_batch` function is the same here as in chapter 5, except that we are only interested in optimizing the last token `model(input_batch)[:, -1, :]` instead of all tokens `model(input_batch)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2f1e9547-806c-41a9-8aba-3b2822baabe4",
   "metadata": {
    "id": "2f1e9547-806c-41a9-8aba-3b2822baabe4"
   },
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)[:, -1, :]  # Logits of last output token\n",
    "    loss = torch.nn.functional.cross_entropy(logits, target_batch)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a013aab9-f854-4866-ad55-5b8350adb50a",
   "metadata": {},
   "source": [
    "The `calc_loss_loader` is exactly the same as in chapter 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b7b83e10-5720-45e7-ac5e-369417ca846b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as in chapter 5\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # Reduce the number of batches to match the total number of batches in the data loader\n",
    "        # if num_batches exceeds the number of batches in the data loader\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56826ecd-6e74-40e6-b772-d3541e585067",
   "metadata": {},
   "source": [
    "- Using the `calc_closs_loader`, we compute the initial training, validation, and test set losses before we start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f6f00e53-5beb-4e64-b147-f26fd481c6ff",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f6f00e53-5beb-4e64-b147-f26fd481c6ff",
    "outputId": "49df8648-9e38-4314-854d-9faacd1b2e89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 2.453\n",
      "Validation loss: 2.583\n",
      "Test loss: 2.322\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet\n",
    "    train_loss = calc_loss_loader(train_loader, model, device, num_batches=5)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device, num_batches=5)\n",
    "    test_loss = calc_loss_loader(test_loader, model, device, num_batches=5)\n",
    "\n",
    "print(f\"Training loss: {train_loss:.3f}\")\n",
    "print(f\"Validation loss: {val_loss:.3f}\")\n",
    "print(f\"Test loss: {test_loss:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04b980b-e583-4f62-84a0-4edafaf99d5d",
   "metadata": {},
   "source": [
    "- In the next section, we train the model to improve the loss values and consequently the classification accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456ae0fd-6261-42b4-ab6a-d24289953083",
   "metadata": {
    "id": "456ae0fd-6261-42b4-ab6a-d24289953083"
   },
   "source": [
    "## 6.7 Finetuning the model on supervised data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9b099b-0829-4f72-8a2b-4363e3497026",
   "metadata": {},
   "source": [
    "- In this section, we define and use the training function to improve the classification accuracy of the model\n",
    "- The `train_classifier_simple` function below is practically the same as the `train_model_simple` function we used for pretraining the model in chapter 5\n",
    "- The only two differences are that we now \n",
    "  1. track the number of training examples seen (`examples_seen`) instead of the number of tokens seen\n",
    "  2. calculate the accuracy after each epoch instead of printing a sample text after each epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979b6222-1dc2-4530-9d01-b6b04fe3de12",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch06_compressed/training-loop.webp?1\" width=500px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "Csbr60to50FL",
   "metadata": {
    "id": "Csbr60to50FL"
   },
   "outputs": [],
   "source": [
    "# Overall the same as `train_model_simple` in chapter 5\n",
    "def train_classifier_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                            eval_freq, eval_iter):\n",
    "    # Initialize lists to track losses and examples seen\n",
    "    train_losses, val_losses, train_accs, val_accs = [], [], [], []\n",
    "    examples_seen, global_step = 0, -1\n",
    "\n",
    "    # Main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward() # Calculate loss gradients\n",
    "            optimizer.step() # Update model weights using loss gradients\n",
    "            examples_seen += input_batch.shape[0] # New: track examples instead of tokens\n",
    "            global_step += 1\n",
    "\n",
    "            # Optional evaluation step\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "        # Calculate accuracy after each epoch\n",
    "        train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "        print(f\"Training accuracy: {train_accuracy*100:.2f}% | \", end=\"\")\n",
    "        print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "        train_accs.append(train_accuracy)\n",
    "        val_accs.append(val_accuracy)\n",
    "\n",
    "    return train_losses, val_losses, train_accs, val_accs, examples_seen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9624cb30-3e3a-45be-b006-c00475b58ae8",
   "metadata": {},
   "source": [
    "- The `evaluate_model` function used in the `train_classifier_simple` is the same as the one we used in chapter 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bcc7bc04-6aa6-4516-a147-460e2f466eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as chapter 5\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e807bfe9-364d-46b2-9e25-3b000c3ef6f9",
   "metadata": {},
   "source": [
    "- The training takes about 5 minutes on a M3 MacBook Air laptop computer and less than half a minute on a V100 or A100 GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "X7kU3aAj7vTJ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X7kU3aAj7vTJ",
    "outputId": "504a033e-2bf8-41b5-a037-468309845513"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 2.153, Val loss 2.392\n",
      "Ep 1 (Step 000050): Train loss 0.617, Val loss 0.637\n",
      "Ep 1 (Step 000100): Train loss 0.523, Val loss 0.557\n",
      "Training accuracy: 70.00% | Validation accuracy: 72.50%\n",
      "Ep 2 (Step 000150): Train loss 0.561, Val loss 0.489\n",
      "Ep 2 (Step 000200): Train loss 0.419, Val loss 0.397\n",
      "Ep 2 (Step 000250): Train loss 0.409, Val loss 0.353\n",
      "Training accuracy: 82.50% | Validation accuracy: 85.00%\n",
      "Ep 3 (Step 000300): Train loss 0.333, Val loss 0.320\n",
      "Ep 3 (Step 000350): Train loss 0.340, Val loss 0.306\n",
      "Training accuracy: 90.00% | Validation accuracy: 90.00%\n",
      "Ep 4 (Step 000400): Train loss 0.136, Val loss 0.200\n",
      "Ep 4 (Step 000450): Train loss 0.153, Val loss 0.132\n",
      "Ep 4 (Step 000500): Train loss 0.222, Val loss 0.137\n",
      "Training accuracy: 100.00% | Validation accuracy: 97.50%\n",
      "Ep 5 (Step 000550): Train loss 0.207, Val loss 0.143\n",
      "Ep 5 (Step 000600): Train loss 0.083, Val loss 0.074\n",
      "Training accuracy: 100.00% | Validation accuracy: 97.50%\n",
      "Training completed in 1.21 minutes.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 5\n",
    "train_losses, val_losses, train_accs, val_accs, examples_seen = train_classifier_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=50, eval_iter=5,\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1261bf90-3ce7-4591-895a-044a05538f30",
   "metadata": {},
   "source": [
    "- Similar to chapter 5, we use matplotlib to plot the loss function for the training and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cURgnDqdCeka",
   "metadata": {
    "id": "cURgnDqdCeka"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_values(epochs_seen, examples_seen, train_values, val_values, label=\"loss\"):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "    # Plot training and validation loss against epochs\n",
    "    ax1.plot(epochs_seen, train_values, label=f\"Training {label}\")\n",
    "    ax1.plot(epochs_seen, val_values, linestyle=\"-.\", label=f\"Validation {label}\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(label.capitalize())\n",
    "    ax1.legend()\n",
    "\n",
    "    # Create a second x-axis for examples seen\n",
    "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
    "    ax2.plot(examples_seen, train_values, alpha=0)  # Invisible plot for aligning ticks\n",
    "    ax2.set_xlabel(\"Examples seen\")\n",
    "\n",
    "    fig.tight_layout()  # Adjust layout to make room\n",
    "    plt.savefig(f\"{label}-plot.pdf\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "OIqRt466DiGk",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 307
    },
    "id": "OIqRt466DiGk",
    "outputId": "b16987cf-0001-4652-ddaf-02f7cffc34db"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "examples_seen_tensor = torch.linspace(0, examples_seen, len(train_losses))\n",
    "\n",
    "plot_values(epochs_tensor, examples_seen_tensor, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd28174-1836-44ba-b6c0-7e0be774fadc",
   "metadata": {},
   "source": [
    "- Above, based on the downward slope, we see that the model learns well\n",
    "- Furthermore, the fact that the training and validation loss are very close indicates that the model does not tend to overfit the training data\n",
    "- Similarly, we can plot the accuracy below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "yz8BIsaF0TUo",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 307
    },
    "id": "yz8BIsaF0TUo",
    "outputId": "3a7ed967-1f2a-4c6d-f4a3-0cc8cc9d6c5f"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_accs))\n",
    "examples_seen_tensor = torch.linspace(0, examples_seen, len(train_accs))\n",
    "\n",
    "plot_values(epochs_tensor, examples_seen_tensor, train_accs, val_accs, label=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90aba699-21bc-42de-a69c-99f370bb0363",
   "metadata": {},
   "source": [
    "- Based on the accuracy plot above, we can see that the model achieves a relatively high training and validation accuracy after epochs 4 and 5\n",
    "- However, we have to keep in mind that we specified `eval_iter=5` in the training function earlier, which means that we only estimated the training and validation set performances\n",
    "- We can compute the training, validation, and test set performances over the complete dataset as follows below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "UHWaJFrjY0zW",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UHWaJFrjY0zW",
    "outputId": "e111e6e6-b147-4159-eb9d-19d4e809ed34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 97.21%\n",
      "Validation accuracy: 97.32%\n",
      "Test accuracy: 95.67%\n"
     ]
    }
   ],
   "source": [
    "train_accuracy = calc_accuracy_loader(train_loader, model, device)\n",
    "val_accuracy = calc_accuracy_loader(val_loader, model, device)\n",
    "test_accuracy = calc_accuracy_loader(test_loader, model, device)\n",
    "\n",
    "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6882649f-dc7b-401f-84d2-024ff79c74a1",
   "metadata": {},
   "source": [
    "- We can see that the training and validation set performances are practically identical\n",
    "- However, based on the slightly lower test set performance, we can see that the model overfits the training data to a very small degree, as well as the validation data that has been used for tweaking some of the hyperparameters, such as the learning rate\n",
    "- This is normal, however, and this gap could potentially be further reduced by increasing the model's dropout rate (`drop_rate`) or the `weight_decay` in the optimizer setting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74d9ad7-3ec1-450e-8c9f-4fc46d3d5bb0",
   "metadata": {},
   "source": [
    "## 6.8 Using the LLM as a spam classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ebcfa2-479e-408b-9cf0-7421f6144855",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch06_compressed/overview-4.webp\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5408e6-83e4-4e5a-8503-c2fba6073f31",
   "metadata": {},
   "source": [
    "- Finally, let's use the finetuned GPT model in action\n",
    "- The `classify_review` function below implements the data preprocessing steps similar to the `SpamDataset` we implemented earlier\n",
    "- Then, the function returns the predicted integer class label from the model and returns the corresponding class name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "aHdn6xvL-IW5",
   "metadata": {
    "id": "aHdn6xvL-IW5"
   },
   "outputs": [],
   "source": [
    "def classify_review(text, model, tokenizer, device, max_length=None, pad_token_id=50256):\n",
    "    model.eval()\n",
    "\n",
    "    # Prepare inputs to the model\n",
    "    input_ids = tokenizer.encode(text)\n",
    "    supported_context_length = model.pos_emb.weight.shape[0]\n",
    "    # Note: In the book, this was originally written as pos_emb.weight.shape[1] by mistake\n",
    "    # It didn't break the code but would have caused unnecessary truncation (to 768 instead of 1024)\n",
    "\n",
    "    # Truncate sequences if they too long\n",
    "    input_ids = input_ids[:min(max_length, supported_context_length)]\n",
    "\n",
    "    # Pad sequences to the longest sequence\n",
    "    input_ids += [pad_token_id] * (max_length - len(input_ids))\n",
    "    input_tensor = torch.tensor(input_ids, device=device).unsqueeze(0) # add batch dimension\n",
    "\n",
    "    # Model inference\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_tensor)[:, -1, :]  # Logits of the last output token\n",
    "    predicted_label = torch.argmax(logits, dim=-1).item()\n",
    "\n",
    "    # Return the classified result\n",
    "    return \"spam\" if predicted_label == 1 else \"not spam\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29682d8-a899-4d9b-b973-f8d5ec68172c",
   "metadata": {},
   "source": [
    "- Let's try it out on a few examples below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "apU_pf51AWSV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "apU_pf51AWSV",
    "outputId": "d0fde0a5-e7a3-4dbe-d9c5-0567dbab7e62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spam\n"
     ]
    }
   ],
   "source": [
    "text_1 = (\n",
    "    \"You are a winner you have been specially\"\n",
    "    \" selected to receive $1000 cash or a $2000 award.\"\n",
    ")\n",
    "\n",
    "print(classify_review(\n",
    "    text_1, model, tokenizer, device, max_length=train_dataset.max_length\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1g5VTOo_Ajs5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1g5VTOo_Ajs5",
    "outputId": "659b08eb-b6a9-4a8a-9af7-d94c757e93c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not spam\n"
     ]
    }
   ],
   "source": [
    "text_2 = (\n",
    "    \"Hey, just wanted to check if we're still on\"\n",
    "    \" for dinner tonight? Let me know!\"\n",
    ")\n",
    "\n",
    "print(classify_review(\n",
    "    text_2, model, tokenizer, device, max_length=train_dataset.max_length\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf736e39-0d47-40c1-8d18-1f716cf7a81e",
   "metadata": {},
   "source": [
    "- Finally, let's save the model in case we want to reuse the model later without having to train it again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "mYnX-gI1CfQY",
   "metadata": {
    "id": "mYnX-gI1CfQY"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"review_classifier.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba78cf7c-6b80-4f71-a50e-3ccc73839af6",
   "metadata": {},
   "source": [
    "- Then, in a new session, we could load the model as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cc4e68a5-d492-493b-87ef-45c475f353f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_state_dict = torch.load(\"review_classifier.pth\", map_location=device, weights_only=True)\n",
    "model.load_state_dict(model_state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b70ac71-234f-4eeb-b33d-c62726d50cd4",
   "metadata": {
    "id": "5b70ac71-234f-4eeb-b33d-c62726d50cd4"
   },
   "source": [
    "## Summary and takeaways"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafdc910-d616-47ab-aa85-f90c6e7ed80e",
   "metadata": {},
   "source": [
    "- See the [./gpt_class_finetune.py](./gpt_class_finetune.py) script, a self-contained script for classification finetuning\n",
    "- You can find the exercise solutions in [./exercise-solutions.ipynb](./exercise-solutions.ipynb)\n",
    "- In addition, interested readers can find an introduction to parameter-efficient training with low-rank adaptation (LoRA) in [appendix E](../../appendix-E)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "V100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "llms-from-scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
